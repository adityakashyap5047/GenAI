{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d947fb73",
   "metadata": {},
   "source": [
    "#### Getting started With Langchain And Open AI\n",
    "\n",
    "In this quickstart we'll see how to:\n",
    "\n",
    "- Get setup with LangChain, LangSmith and LangServe\n",
    "- Use the most basic and common components of LangChain: prompt templates, models, and output parsers.\n",
    "- Build a simple application with LangChain\n",
    "- Trace your application with LangSmith\n",
    "- Serve your application with LangServe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fec261c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY')\n",
    "os.environ['LANGCHAIN_API_KEY'] = os.getenv('LANGCHAIN_API_KEY')    ### For LangSmith Tracking\n",
    "os.environ['LANGCHAIN_TRACING_V2'] = 'true'  ### For LangSmith Tracking\n",
    "os.environ['LANGCHAIN_PROJECT'] = os.getenv('LANGCHAIN_PROJECT')  ### For LangSmith Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc076c98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client=<openai.resources.chat.completions.completions.Completions object at 0x000001B977CE4850> async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001B977CE73A0> root_client=<openai.OpenAI object at 0x000001B976CE2A70> root_async_client=<openai.AsyncOpenAI object at 0x000001B977CE48B0> model_name='gpt-4o' model_kwargs={} openai_api_key=SecretStr('**********')\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o\")\n",
    "print(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "331e15bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Input and get response from LLM\n",
    "\n",
    "result = llm.invoke(\"What is generative AI?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7456fdde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Generative AI refers to a category of artificial intelligence systems designed to generate new content, such as text, images, music, or code, often by learning patterns from existing data. These systems use models like Generative Adversarial Networks (GANs), Variational Autoencoders (VAEs), and transformer-based architectures like GPT (Generative Pre-trained Transformers). \\n\\nThe core idea is to train models on large datasets so they can create outputs that mimic the input data in creative or useful ways. For example, they can produce natural language responses (as in chatbots), create art, simulate realistic human faces, compose music, or generate code. Generative AI has applications across various fields, including entertainment, design, and even healthcare, for tasks like drug discovery or creating personalized medicine.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 159, 'prompt_tokens': 13, 'total_tokens': 172, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_f5bdcc3276', 'id': 'chatcmpl-BTpS0izAH4VhFdM400i8ISLa3P0mv', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='run--2e8f259a-324b-47cd-b4b1-0ef80cc813a9-0' usage_metadata={'input_tokens': 13, 'output_tokens': 159, 'total_tokens': 172, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b35311f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are an expert AI Engineer. Provide me answers based on the questions.'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Chatprompt template\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are an expert AI Engineer. Provide me answers based on the questions.\"),\n",
    "    (\"user\", \"{input}\")\n",
    "])\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8ebeeb02",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | llm    ### The 'or' here will do is that we need to combine prompt along with LLM and that becomes a chain. and whenever we call the chain, it will call the prompt and then LLM.\n",
    "\n",
    "response = chain.invoke({\"input\": \"Can you tell me about Langsmith\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3da13b4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Langsmith is a tool developed to enhance the development, testing, and monitoring of applications built with large language models (LLMs). It is particularly beneficial for applications that use LangChain, a popular framework for creating applications with LLMs. Langsmith allows developers to:\\n\\n1. **Trace and Debug**: It provides a way to trace the execution of LLM applications, logging input, output, and any intermediate steps, which helps in debugging and understanding how the models work internally.\\n   \\n2. **Evaluation**: By facilitating easy evaluation of model outputs, Langsmith helps developers ensure their LLMs are performing as expected. It offers functionality to set up benchmarks and perform qualitative and quantitative assessments.\\n\\n3. **Experimentation**: Developers can experiment with different models, prompts, or configurations to see how changes impact performance, making it easier to optimize the application.\\n\\n4. **Monitoring**: Langsmith provides tools to monitor applications in production, offering insights on usage patterns, performance metrics, and potential bottlenecks or issues.\\n\\nOverall, Langsmith aims to streamline the workflow for developers working with LLMs by providing robust tools for managing and refining their models throughout the development lifecycle. It integrates seamlessly with LangChain, making it a powerful add-on for applications built using that framework.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 258, 'prompt_tokens': 33, 'total_tokens': 291, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_90122d973c', 'id': 'chatcmpl-BTph15QCsZ52COaDaam6pmHCEtGaq', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--71fb653c-f3c2-43a2-8beb-f52ac1b984c1-0', usage_metadata={'input_tokens': 33, 'output_tokens': 258, 'total_tokens': 291, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "49782646",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_core.messages.ai.AIMessage"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3806df81",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
